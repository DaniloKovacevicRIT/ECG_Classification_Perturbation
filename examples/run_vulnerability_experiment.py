"""
Driver script for the vulnerability landscape experiment described in docs/main_experiments.md.

The script expects:
  * A NumPy .npz file containing `X_test` and `y_test_enc`.
  * A TensorFlow SavedModel or H5 checkpoint for the CNN trained in StartingFile.ipynb.

Example:
    python examples/run_vulnerability_experiment.py ^
        --data data ^
        --model models/ptb_cnn ^
        --output results/vulnerability
"""

from __future__ import annotations

import argparse
import logging
import pathlib
import sys
from typing import Sequence, Tuple

ROOT_DIR = pathlib.Path(__file__).resolve().parents[1]
VENV_SITE = ROOT_DIR / ".venv" / "Lib" / "site-packages"
if VENV_SITE.exists():
    sys.path.insert(0, str(VENV_SITE))

sys.path.append(str(ROOT_DIR))

import numpy as np
import tensorflow as tf

from perturbations.config import CLASS_NAMES
from perturbations.evaluation import (
    refine_window_strengths_with_binary_search,
    results_to_dataframe,
    run_saliency_guided_attacks,
    save_vulnerability_results,
    select_eval_subset,
    summarize_minimal_strength_per_sample,
)
from perturbations.tf_utils import configure_tensorflow_device, tf_device_scope
from visualization import (
    plot_asr_time_class_heatmap,
    plot_asr_vs_time,
    plot_robust_fraction_by_class,
    plot_strength_boxplot_by_class,
    plot_strength_histogram,
)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run saliency-guided vulnerability experiments.")
    parser.add_argument(
        "--data",
        type=pathlib.Path,
        default=pathlib.Path("data") / "ptb_eval.npz",
        help="Path to NPZ with 'X_test' and 'y_test_enc' arrays.",
    )
    parser.add_argument(
        "--model",
        type=pathlib.Path,
        default=pathlib.Path("models") / "ptb_cnn.keras",
        help="Path to TensorFlow Keras/H5 checkpoint or SavedModel directory.",
    )
    parser.add_argument(
        "--output",
        type=pathlib.Path,
        default=pathlib.Path("results") / "vulnerability",
        help="Directory to store logs, tables, and figures.",
    )
    parser.add_argument("--max-samples", type=int, default=1000, help="Evaluation subset size.")
    parser.add_argument(
        "--random-state", type=int, default=42, help="Random seed for subset selection."
    )
    parser.add_argument("--window-seconds", type=float, default=2.0, help="Window duration in seconds.")
    parser.add_argument("--top-k", type=int, default=3, help="Number of windows per sample.")
    parser.add_argument("--center-stride", type=int, default=5, help="Stride in samples for window search.")
    parser.add_argument(
        "--max-overlap",
        type=float,
        default=0.25,
        help="Maximum allowed fractional overlap between selected windows.",
    )
    parser.add_argument("--seed-base", type=int, default=0, help="Base seed for RNG determinism.")
    parser.add_argument(
        "--skip-binary-search",
        action="store_true",
        help="Disable binary search refinement of minimal strengths.",
    )
    parser.add_argument("--binary-iters", type=int, default=6, help="Binary search iterations.")
    parser.add_argument(
        "--binary-tolerance",
        type=float,
        default=0.01,
        help="Binary search tolerance on strength.",
    )
    parser.add_argument(
        "--strength-schedule",
        type=str,
        default="0.10,0.20,0.35,0.50",
        help="Comma-separated schedule of strengths for the coarse sweep.",
    )
    parser.add_argument("--fs", type=float, default=100.0, help="Sampling rate in Hz.")
    parser.add_argument(
        "--device",
        type=str,
        choices=["auto", "cpu", "gpu"],
        default="auto",
        help="TensorFlow device to run on ('auto' prefers GPU when available).",
    )
    return parser.parse_args()


def load_eval_arrays(path: pathlib.Path) -> Tuple[np.ndarray, np.ndarray]:
    data = np.load(path)
    if "X_test" not in data or "y_test_enc" not in data:
        raise ValueError("NPZ file must contain 'X_test' and 'y_test_enc'.")
    return data["X_test"], data["y_test_enc"]


def parse_strength_schedule(text: str) -> Sequence[float]:
    values = [float(tok.strip()) for tok in text.split(",") if tok.strip()]
    if not values:
        raise ValueError("Strength schedule must contain at least one value.")
    return values


def generate_figures(df_windows, df_samples, output_dir: pathlib.Path) -> None:
    output_dir.mkdir(parents=True, exist_ok=True)
    figure_specs = [
        (
            plot_asr_vs_time,
            {"df_windows": df_windows, "bin_width": 0.5},
            output_dir / "asr_vs_time.png",
        ),
        (
            plot_asr_time_class_heatmap,
            {"df_windows": df_windows, "class_names": CLASS_NAMES, "bin_width": 0.5},
            output_dir / "asr_time_class_heatmap.png",
        ),
        (
            plot_strength_histogram,
            {"df_samples": df_samples},
            output_dir / "strength_histogram.png",
        ),
        (
            plot_strength_boxplot_by_class,
            {"df_samples": df_samples, "class_names": CLASS_NAMES},
            output_dir / "strength_boxplot_by_class.png",
        ),
        (
            plot_robust_fraction_by_class,
            {"df_samples": df_samples, "class_names": CLASS_NAMES},
            output_dir / "robust_fraction_by_class.png",
        ),
    ]

    for fn, kwargs, path in figure_specs:
        try:
            kwargs = dict(kwargs)
            kwargs["show"] = False
            kwargs["save_path"] = str(path)
            fn(**kwargs)
            logging.info("Saved %s", path)
        except ValueError as exc:
            logging.warning("Skipping %s: %s", fn.__name__, exc)


def main() -> None:
    args = parse_args()
    logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
    device = configure_tensorflow_device(args.device)
    if not args.data.exists():
        raise FileNotFoundError(
            f"{args.data} not found. Run 'python examples/prepare_ptb_eval_data.py --output {args.data}' "
            f"or 'python examples/train_ptb_cnn.py --export-eval {args.data}' to generate it."
        )
    if not args.model.exists():
        raise FileNotFoundError(
            f"Model path {args.model} not found. Train one via "
            "'python examples/train_ptb_cnn.py --output models/ptb_cnn'."
        )

    logging.info("Loading evaluation arrays from %s", args.data)
    X_test, y_test_enc = load_eval_arrays(args.data)

    with tf_device_scope(device):
        logging.info("Loading model from %s", args.model)
        model = tf.keras.models.load_model(args.model)

        eval_indices = select_eval_subset(
            y_test_enc, max_samples=args.max_samples, random_state=args.random_state
        )
        logging.info("Selected %d evaluation samples.", len(eval_indices))

        schedule = parse_strength_schedule(args.strength_schedule)
        logging.info("Running saliency-guided attacks...")
        attack_results, df_windows = run_saliency_guided_attacks(
            X_test,
            y_test_enc,
            model,
            eval_indices,
            fs=args.fs,
            strength_schedule=schedule,
            window_seconds=args.window_seconds,
            top_k=args.top_k,
            center_stride=args.center_stride,
            max_overlap=args.max_overlap,
            seed_base=args.seed_base,
        )
        df_attempts = results_to_dataframe(
            attack_results, include_probabilities=True, include_vectors=True
        )

        if not args.skip_binary_search:
            logging.info("Refining strengths with binary search...")
            df_windows, binary_results = refine_window_strengths_with_binary_search(
                df_windows,
                df_attempts,
                model,
                X_test,
                y_test_enc,
                fs=args.fs,
                tolerance=args.binary_tolerance,
                max_iters=args.binary_iters,
                seed_base=args.seed_base,
            )
            if binary_results:
                attack_results.extend(binary_results)
                df_attempts = results_to_dataframe(
                    attack_results, include_probabilities=True, include_vectors=True
                )
        else:
            logging.info("Skipping binary search refinement.")

        df_samples = summarize_minimal_strength_per_sample(df_windows)

    save_vulnerability_results(df_attempts, df_windows, df_samples, root=str(args.output))
    generate_figures(df_windows, df_samples, args.output)
    logging.info("Experiment artifacts saved to %s", args.output)


if __name__ == "__main__":
    main()
